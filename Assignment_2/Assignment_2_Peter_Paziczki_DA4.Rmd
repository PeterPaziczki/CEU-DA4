---
title: "Homework assignment 2"
author: "Peter Paziczki"
date: "2018 February 18"
output:
  pdf_document: default
  html_document: default
subtitle: 'Data Analysis 4: Prediction Analytics with Introduction to Machine Learning
  2017/2018 Winter'
---

<style>
body {
text-align: justify}
</style>

```{r setup, include=FALSE}
rm(list=ls())
knitr::opts_chunk$set(warning = FALSE, echo = FALSE)
options(scipen=999) # disabling scientific notation
```

```{r include=FALSE}
# loading necessary 
library(data.table)
library(ggplot2)
library(tidyr)
library(DescTools) # for BrierScore function
library(pROC) # for roc function
library(dplyr)
library(randomForest)
library(pander)
#library(lattice)
#library(caret)
#library(glmnet)
```

### 1. Predicting firm default

We have been provided the bisnode_all.csv dataset containing more than 150.000 observations, covering financial data from 2011 to 2016, management related information and further information about employment and industrial classification. Our task is to define firm default and build models that can predict it.

```{r include=FALSE}
## Loading the bisnode data
bisnode_all <- fread("bisnode_all.csv")
```

```{r include=FALSE}
## Creating a table with unique company IDs and sales values from all years
data_sales <- bisnode_all[, c("comp_id", "sales", "year")]
data_sales <- spread(data_sales, key = year, value = sales)
names(data_sales) <- c("comp_id", "sales_2011", "sales_2012", "sales_2013", "sales_2014", "sales_2015", "sales_2016")

## Dropping the year 2016, because when the data was gathered, not all the the companies have submitted their financial records for 2016.
data_sales[,"sales_2016" := NULL]

## Keeping companies that have no NA records in years 2011 to 2013
data <- data_sales[complete.cases(data_sales[,c("sales_2011", "sales_2012", "sales_2013")]),]
```

```{r include=FALSE}
## computing the percentage changes in sales from year to year
data[, sales_change_11_12 := sales_2012/sales_2011]
data[, sales_change_12_13 := sales_2013/sales_2012]
data[, sales_change_13_14 := sales_2014/sales_2013]
data[, sales_change_14_15 := sales_2015/sales_2014]
```

```{r include=FALSE}
## Creating binary variables

#data[, sales_2011_is_NA := factor(ifelse(is.na(data[,sales_2011]), 1, 0))]
#data[, sales_2012_is_NA := factor(ifelse(is.na(data[,sales_2012]), 1, 0))]
#data[, sales_2013_is_NA := factor(ifelse(is.na(data[,sales_2013]), 1, 0))]
data[, sales_2014_is_NA := factor(ifelse(is.na(data[,sales_2014]), 1, 0))]
data[, sales_2015_is_NA := factor(ifelse(is.na(data[,sales_2015]), 1, 0))]
#data[, sales_2016_is_NA := factor(ifelse(is.na(data[,sales_2016]), 1, 0))]
```

#### 1.1 Defining firm default

I am going to consider a firm defaulted if there are no records about its sales results in 2014 and 2015. Having no records means that it is either zero and / or missing. I could see many examples of missing sales records of one year, but two missing records in row is enough for me to say that the company has defaulted.

It also means that I cannot work with companies founded later than 2013. Companies that had zero as record of sales in all years from 2011 to 2013 have also been dropped, those are considered as inactive, irrelevant for our study. I have dropped the year 2016, because when the data was gathered, not all the the companies have submitted their financial records for 2016.

I created a binary flag called `defaulted`, 1 indicates that the company has defaulted.

#### 1.2 Data preparation

I need to find and create relevant predictors to able to build a model with actual predicting power. Let

```{r include=FALSE}
## Defining firm default

#data[,defaulted := factor(ifelse(sales_2014_is_NA == 1 & sales_2015_is_NA == 1, 1, 0))]
data[,defaulted := ifelse(sales_2014_is_NA == 1 & sales_2015_is_NA == 1 | 
                                        sales_2014 == 0 & sales_2015 == 0 | 
                                        sales_2014 == 0 & sales_2015_is_NA == 1 |
                                        sales_2014_is_NA == 1 & sales_2015 == 0 , 1, 0)]
data[,f_defaulted := factor(defaulted)] # random forest can operate with factor but cannot with characters
data <- data[!(sales_2011 == 0 & sales_2012 == 0 & sales_2013 == 0
                         #& sales_2014 == 0 & sales_2015 == 0
                         )]

## Creating a list of companies that we are going to include
list_of_companies <- data[, unique(comp_id)]
```



```{r include=FALSE}
## Creating a table with unique company IDs and labor values from all years
data_labor_avg <- bisnode_all[, c("comp_id", "labor_avg", "year")]
data_labor_avg <- spread(data_labor_avg, key = year, value = labor_avg)
is.data.table(data_labor_avg)
names(data_labor_avg) <- c("comp_id", "labor_2011", "labor_2012", "labor_2013", "labor_2014", "labor_2015", "labor_2016")
str(data_labor_avg)

## Including only the companies that were dropped previously
data_labor_avg <- data_labor_avg[comp_id %in% list_of_companies]

## Computing the percentage changes in sales from year to year
data_labor_avg[, labor_change_11_12 := labor_2012/labor_2011]
data_labor_avg[, labor_change_12_13 := labor_2013/labor_2012]
data_labor_avg[, labor_change_13_14 := labor_2014/labor_2013]
data_labor_avg[, labor_change_14_15 := labor_2015/labor_2014]

data_labor_avg[,"labor_2016" := NULL]

## Merging it to the bigger table
data <- merge(data, data_labor_avg, all = TRUE)

#data_labor_avg <- data_labor_avg[complete.cases(data_wide[,2:4]),]
```

I am expecting company age to be an relevant predictor, I am assuming that the older a company, the more stable it is and the less likely it is to default.

```{r include=FALSE}
## Adding company age
data_founded_year <- bisnode_all[, c("comp_id", "founded_year", "year")]
data_founded_year <- spread(data_founded_year, key = year, value = founded_year)
names(data_founded_year) <- c("comp_id", "year_2011", "year_2012", "year_2013", "year_2014", "year_2015", "year_2016")

data_founded_year <- data_founded_year[comp_id %in% list_of_companies]

data_founded_year[ , founded_year :=rowMeans(.SD, na.rm = TRUE), 
                   .SDcols = c("year_2011", "year_2012", "year_2013", "year_2014", "year_2015", "year_2016")]

data <- merge(data, data_founded_year, all = TRUE)

#test <- data
#test <- cbind(test,data_founded_year[,"founded_year"])

data <- data[,age := 2015 - founded_year]

## creating 5 years age groups
data[,agegroup:=cut(age, seq(0,35,5), right = F,include.lowest = T )]

## creating 1 year long age groups
data[,agegi:=cut(age, seq(0,35,1), right = F,include.lowest = T )]
data[,agesq:=age^2]
data[,agecu:=age^3]
```

```{r include=FALSE}
## Adding region
#data_region <- bisnode_all[, c("comp_id", "region_m", "year")]
#data_region <- spread(data_region, key = year, value = region_m)
#names(data_founded_year) <- c("comp_id", "year_2011", "year_2012", "year_2013", "year_2014", "year_2015", "year_2016")

data_region <- bisnode_all[, unique(region_m), by = comp_id]
colnames(data_region) <- c("comp_id", "region")
data_region <- data_region[comp_id %in% list_of_companies]
data <- merge(data, data_region, all = TRUE)
data[,f_region := factor(region)]
```

```{r include=FALSE}
## Creating a table with unique company IDs and profit loss values from all years
data_profit_loss_year <- bisnode_all[, c("comp_id", "profit_loss_year", "year")]
data_profit_loss_year <- spread(data_profit_loss_year, key = year, value = profit_loss_year)
names(data_profit_loss_year) <- c("comp_id", "profit_loss_2011", "profit_loss_2012", "profit_loss_2013", "profit_loss_2014", "profit_loss_2015", "profit_loss_2016")

## Dropping the year 2016, because when the data was gathered, not all the the companies have submitted their financial records for 2016.
data_profit_loss_year[,"profit_loss_2016" := NULL]
data_profit_loss_year <- data_profit_loss_year[comp_id %in% list_of_companies]

##############################################################################

## Creating a table with unique company IDs and shared equity values from all years
data_share_eq <- bisnode_all[, c("comp_id", "share_eq", "year")]
data_share_eq <- spread(data_share_eq, key = year, value = share_eq)
names(data_share_eq) <- c("comp_id", "share_eq_2011", "share_eq_2012", "share_eq_2013", "share_eq_2014", "share_eq_2015", "share_eq_2016")

## Dropping the year 2016, because when the data was gathered, not all the the companies have submitted their financial records for 2016.
data_share_eq[,"share_eq_2016" := NULL]
data_share_eq <- data_share_eq[comp_id %in% list_of_companies]

##############################################################################

## Creating a table with unique company IDs and income before tax values from all years
data_inc_bef_tax <- bisnode_all[, c("comp_id", "inc_bef_tax", "year")]
data_inc_bef_tax <- spread(data_inc_bef_tax, key = year, value = inc_bef_tax)
names(data_inc_bef_tax) <- c("comp_id", "income_bef_tax_2011", "income_bef_tax_2012", "income_bef_tax_2013", "income_bef_tax_2014", "income_bef_tax_2015", "income_bef_tax_2016")

## Dropping the year 2016, because when the data was gathered, not all the the companies have submitted their financial records for 2016.
data_inc_bef_tax[,"income_bef_tax_2016" := NULL]
data_inc_bef_tax <- data_inc_bef_tax[comp_id %in% list_of_companies]

##############################################################################

## Creating a table with unique company IDs and current assets values from all years
data_curr_assets <- bisnode_all[, c("comp_id", "curr_assets", "year")]
data_curr_assets <- spread(data_curr_assets, key = year, value = curr_assets)
names(data_curr_assets) <- c("comp_id", "curr_assets_2011", "curr_assets_2012", "curr_assets_2013", "curr_assets_2014", "curr_assets_2015", "curr_assets_2016")

## Dropping the year 2016, because when the data was gathered, not all the the companies have submitted their financial records for 2016.
data_curr_assets[,"curr_assets_2016" := NULL]
data_curr_assets <- data_curr_assets[comp_id %in% list_of_companies]

##############################################################################

## Creating a table with unique company IDs and current assets values from all years
data_fixed_assets <- bisnode_all[, c("comp_id", "fixed_assets", "year")]
data_fixed_assets <- spread(data_fixed_assets, key = year, value = fixed_assets)
names(data_fixed_assets) <- c("comp_id", "fixed_assets_2011", "fixed_assets_2012", "fixed_assets_2013", "fixed_assets_2014", "fixed_assets_2015", "fixed_assets_2016")

## Dropping the year 2016, because when the data was gathered, not all the the companies have submitted their financial records for 2016.
data_fixed_assets[,"fixed_assets_2016" := NULL]
data_fixed_assets <- data_fixed_assets[comp_id %in% list_of_companies]

##############################################################################

## Creating a table with unique company IDs and current assets values from all years
data_curr_liab <- bisnode_all[, c("comp_id", "curr_liab", "year")]
data_curr_liab <- spread(data_curr_liab, key = year, value = curr_liab)
names(data_curr_liab) <- c("comp_id", "curr_liab_2011", "curr_liab_2012", "curr_liab_2013", "curr_liab_2014", "curr_liab_2015", "curr_liab_2016")

## Dropping the year 2016, because when the data was gathered, not all the the companies have submitted their financial records for 2016.
data_curr_liab[,"curr_liab_2016" := NULL]
data_curr_liab <- data_curr_liab[comp_id %in% list_of_companies]

data <- merge(data, data_profit_loss_year, all = TRUE)
data <- merge(data, data_share_eq, all = TRUE)
data <- merge(data, data_inc_bef_tax, all = TRUE)
data <- merge(data, data_curr_assets, all = TRUE)
data <- merge(data, data_fixed_assets, all = TRUE)
data <- merge(data, data_curr_liab, all = TRUE)
```

```{r include=FALSE}
## ROE = Net_Income / Shareholders Equity
data[, ROE_2011 := profit_loss_2011 / share_eq_2011]
data[, ROE_2012 := profit_loss_2012 / share_eq_2012]
data[, ROE_2013 := profit_loss_2013 / share_eq_2013]
data[, ROE_2014 := profit_loss_2014 / share_eq_2014]
data[, ROE_2015 := profit_loss_2015 / share_eq_2015]

#library(mltools)
#data[, "ROE_group_2011"] <- bin_data(data$ROE_2011, bins=5, binType = "quantile")
#data[,ROE_group_2011_test:=cut(ROE_group_2011, 5, labels=c(1:5), right = F)]

## ROCE - returnal on capital employed
#bisnode[, ROCE := inc_bef_tax / (curr_assets + fixed_assets - curr_liab)] 

data[, ROCE_2011 := income_bef_tax_2011 / (curr_assets_2011 + fixed_assets_2011 - curr_liab_2011)]
data[, ROCE_2012 := income_bef_tax_2012 / (curr_assets_2012 + fixed_assets_2012 - curr_liab_2012)]
data[, ROCE_2013 := income_bef_tax_2013 / (curr_assets_2013 + fixed_assets_2013 - curr_liab_2013)]
data[, ROCE_2014 := income_bef_tax_2014 / (curr_assets_2014 + fixed_assets_2014 - curr_liab_2014)]
data[, ROCE_2015 := income_bef_tax_2015 / (curr_assets_2015 + fixed_assets_2015 - curr_liab_2015)]
```

```{r include=FALSE}
## Adding industry codes
data_ind <- bisnode_all[, unique(ind), by = comp_id]
colnames(data_ind) <- c("comp_id", "ind")
data_ind <- data_ind[comp_id %in% list_of_companies]

data <- merge(data, data_ind, all = TRUE)
data[,f_ind := factor(ind)]

## Adding industry codes
data_ind2 <- bisnode_all[, unique(ind2), by = comp_id]
colnames(data_ind2) <- c("comp_id", "ind2")
data_ind2 <- data_ind2[comp_id %in% list_of_companies]
data <- merge(data, data_ind2, all = TRUE)
data[,f_ind2 := factor(ind2)]
```

```{r include=FALSE}
data [, size_third := quantile ((curr_assets_2014 + fixed_assets_2014), 0.33, na.rm = TRUE), by = ind]
data [, size_twothird := quantile ((curr_assets_2014 + fixed_assets_2014), 0.66, na.rm = TRUE), by = ind]
data [, size_cat := cut (curr_assets_2014 + fixed_assets_2014, c(
                            0, size_third [1],
                            size_twothird [1], Inf),
                            labels = c("small", "medium", "big")),
                            by = ind]

##########################################################################################

data [, size_third := quantile (((income_bef_tax_2012 - income_bef_tax_2011)/income_bef_tax_2011), 0.33, na.rm = TRUE), by = ind]
data [, size_twothird := quantile (((income_bef_tax_2012 - income_bef_tax_2011)/income_bef_tax_2011), 0.66, na.rm = TRUE), by = ind]
data [, income_cat_2012 := cut (((income_bef_tax_2012 - income_bef_tax_2011)/income_bef_tax_2011), c(
                            -Inf, size_third [1],
                            size_twothird [1], Inf),
                            labels = c("small", "medium", "big")),
                            by = ind]

data [, size_third := quantile (((income_bef_tax_2013 - income_bef_tax_2012)/income_bef_tax_2012), 0.33, na.rm = TRUE), by = ind]
data [, size_twothird := quantile (((income_bef_tax_2013 - income_bef_tax_2012)/income_bef_tax_2012), 0.66, na.rm = TRUE), by = ind]
data [, income_cat_2013 := cut (((income_bef_tax_2013 - income_bef_tax_2012)/income_bef_tax_2012), c(
                            -Inf, size_third [1],
                            size_twothird [1], Inf),
                            labels = c("small", "medium", "big")),
                            by = ind]

data [, size_third := quantile (((income_bef_tax_2014 - income_bef_tax_2013)/income_bef_tax_2013), 0.33, na.rm = TRUE), by = ind]
data [, size_twothird := quantile (((income_bef_tax_2014 - income_bef_tax_2013)/income_bef_tax_2013), 0.66, na.rm = TRUE), by = ind]
data [, income_cat_2014 := cut (((income_bef_tax_2014 - income_bef_tax_2013)/income_bef_tax_2013), c(
                            -Inf, size_third [1],
                            size_twothird [1], Inf),
                            labels = c("small", "medium", "big")),
                            by = ind]

data [, size_third := quantile (((income_bef_tax_2015 - income_bef_tax_2014)/income_bef_tax_2014), 0.33, na.rm = TRUE), by = ind]
data [, size_twothird := quantile (((income_bef_tax_2015 - income_bef_tax_2014)/income_bef_tax_2014), 0.66, na.rm = TRUE), by = ind]
data [, income_cat_2015 := cut (((income_bef_tax_2015 - income_bef_tax_2014)/income_bef_tax_2014), c(
                            -Inf, size_third [1],
                            size_twothird [1], Inf),
                            labels = c("small", "medium", "big")),
                            by = ind]

##########################################################################################

data [, size_third := quantile (((sales_2012 - sales_2011)/sales_2011), 0.33, na.rm = TRUE), by = ind]
data [, size_twothird := quantile (((sales_2012 - sales_2011)/sales_2011), 0.66, na.rm = TRUE), by = ind]
data [, sales_cat_2012 := cut (((sales_2012 - sales_2011)/sales_2011), c(
                            -Inf, size_third [1],
                            size_twothird [1], Inf),
                            labels = c("small", "medium", "big")),
                            by = ind]

data [, size_third := quantile (((sales_2013 - sales_2012)/sales_2012), 0.33, na.rm = TRUE), by = ind]
data [, size_twothird := quantile (((sales_2013 - sales_2012)/sales_2012), 0.66, na.rm = TRUE), by = ind]
data [, sales_cat_2013 := cut (((sales_2013 - sales_2012)/sales_2012), c(
                            -Inf, size_third [1],
                            size_twothird [1], Inf),
                            labels = c("small", "medium", "big")),
                            by = ind]

data [, size_third := quantile (((sales_2014 - sales_2013)/sales_2013), 0.33, na.rm = TRUE), by = ind]
data [, size_twothird := quantile (((sales_2014 - sales_2013)/sales_2013), 0.66, na.rm = TRUE), by = ind]
data [, sales_cat_2014 := cut (((sales_2014 - sales_2013)/sales_2013), c(
                            -Inf, size_third [1],
                            size_twothird [1], Inf),
                            labels = c("small", "medium", "big")),
                            by = ind]

data [, size_third := quantile (((sales_2015 - sales_2014)/sales_2014), 0.33, na.rm = TRUE), by = ind]
data [, size_twothird := quantile (((sales_2015 - sales_2014)/sales_2014), 0.66, na.rm = TRUE), by = ind]
data [, sales_cat_2015 := cut (((sales_2015 - sales_2014)/sales_2014), c(
                            -Inf, size_third [1],
                            size_twothird [1], Inf),
                            labels = c("small", "medium", "big")),
                            by = ind]

##########################################################################################

data [, size_third := quantile (profit_loss_2011 / share_eq_2011, 0.33, na.rm = TRUE), by = ind]
data [, size_twothird := quantile (profit_loss_2011 / share_eq_2011, 0.66, na.rm = TRUE), by = ind]
data [, ROE_2011_cat := cut (profit_loss_2011 / share_eq_2011, c(
                            -Inf, size_third [1],
                            size_twothird [1], Inf),
                            labels = c("small", "medium", "big")),
                            by = ind]

data [, size_third := quantile (profit_loss_2012 / share_eq_2012, 0.33, na.rm = TRUE), by = ind]
data [, size_twothird := quantile (profit_loss_2012 / share_eq_2012, 0.66, na.rm = TRUE), by = ind]
data [, ROE_2012_cat := cut (profit_loss_2012 / share_eq_2012, c(
                            -Inf, size_third [1],
                            size_twothird [1], Inf),
                            labels = c("small", "medium", "big")),
                            by = ind]

data [, size_third := quantile (profit_loss_2013 / share_eq_2013, 0.33, na.rm = TRUE), by = ind]
data [, size_twothird := quantile (profit_loss_2013 / share_eq_2013, 0.66, na.rm = TRUE), by = ind]
data [, ROE_2013_cat := cut (profit_loss_2013 / share_eq_2013, c(
                            -Inf, size_third [1],
                            size_twothird [1], Inf),
                            labels = c("small", "medium", "big")),
                            by = ind]
```

```{r include=FALSE}
## Dropping companies founded in 2014 or later, and companies founded before 1986
data <- data[founded_year<=2013]
data <- data[founded_year>=1986]

## Dropping companies with missing regions
data <- data[region != ""]

## Dropping companies with missing regions
data <- data[!(age == 35)]

## Dropping companies with missing industries
data <- data[ind != ""]

## Dropping zero values to avoid having NAs
data <- data[sales_2011 != 0]
data <- data[sales_2012 != 0]
#data <- data[sales_2013 != 0]
data <- data[income_bef_tax_2011 != 0]
data <- data[income_bef_tax_2012 != 0]
data <- data[share_eq_2011 != 0]
data <- data[share_eq_2012 != 0]
data <- data[share_eq_2013 != 0]
data <- data[profit_loss_2011 != 0]
data <- data[profit_loss_2012 != 0]
data <- data[profit_loss_2013 != 0]

data <- data[complete.cases(data[,"size_cat"])]
data <- data[complete.cases(data[,c("income_bef_tax_2011", "income_bef_tax_2012", "income_bef_tax_2013")])]
data <- data[complete.cases(data[,c("sales_2011", "sales_2012", "sales_2013")])]
data <- data[complete.cases(data[,c("share_eq_2011", "share_eq_2012", "share_eq_2013")])]
#data <- data[complete.cases(data[,c("ROE_2011", "ROE_2012", "ROE_2013", "ROE_2014", "ROE_2015")])]
data <- data[complete.cases(data[,c("profit_loss_2011", "profit_loss_2012","profit_loss_2013")])]
```

**Sales**

Sales plays an important role in our study, so I have computed sales growth for 2011 to 2012 and for 2012 to 2013. The equation was *Current Period Net Sales - Prior Period Net Sales) / Prior Period Net Sales*. Previously I have dropped all the observations that would have NA in sales records in any of the years from 2011 to 2013, but to make sure that that sales growth wouldn't be NA, I have to avoid having zero in the denominator, therefore I have dropped the observations that has zero in sales records for year 2011 and 2012. After calculating sales growth I grouped the sales growth into three categories (by industries) and used that for modelling. It proved to be one of the most performing predictors (see variable importance plot).

**Age**

I am expecting age to be an important predictor. I have computed it using the year the firm was founded in and substracted it from 2015. In addition to that I have cut the observations into 5 years long age groups, please find a table below showing the number of observations in the different groups.

```{r}
pander(data[,.(min_age = min(age) , max_age = max(age), n=.N ), by=agegroup])
```

Later there will be more about finding the best functional form of age variable, but in short, a quadratic from described it the best.

**Region**

Region sounded to be an interesting potential predictor, so I have included that too. Please find a table below showing the regions available and the number of companies in them.

```{r}
pander(data[,.N, by = region])
```

It turned out to be one of the least important predcitors. I had assumption that region could have an effect on firm default, but it is far less relevant than I thought it would be.

**Income before tax**

Similar to sales growth, I also investigated the yearly change in `inc_bef_tax` variable from year 2011 to 2013 and grouped the results into three groups (by industries). It turned out to be an improtant predictor.

**Size**

I defined a way to group the companies by their size. I chose a year and added the `curr_assets` and `fixed_assets` variables and based on the results I have grouped them into three different size groups (by industries). It also turned out to be one of most important predictors (see variable importance plot).

**Firm default**

After the data preparation I am checking, how many companies have been flagged as *defaulted*:

```{r}
pander(table(data$defaulted))
```

I am still checking the number of defaulted companies but by industry (`ind` variable) and age groups (`agegroup` variable).

```{r}
data[,.N, by = list(defaulted,ind)]
data[,.N, by = list(defaulted,agegroup)]
```

Approximately 7% of the firm have been labelled to be defaulted.

**ROE**

I created one additional measure of company performance,Return on Equity (ROE). ROE is by far the most widespread simple tool to measure the net earnings ratio of a company or an investment. The reason I chose ROE is that it is easy to handle and to calculate, I have all the data needed to calculate it, and it is proven to be a good indicator of companies’ profitability. I need just two variables, the Net income (`profit_loss_year`) and Shareholder’s Equity (`share_eq` variable) to do the calculation. Calculating ROE for a given year is a “static” value and I calculated it for years 2011 to 2013.

I grouped the values into three groups (by industries), as I did previously.

#### 1.2 Benchmark model

As a benchmark model I am starting with a logit model with one single predictor, square of age.

```{r}
#####################################################################
# CALIBRATION EXAMPLE
# AGE ONLY

logit <- glm(defaulted ~ poly(age, 2), family = "binomial", data=data)
logit
## storing the probability predictions in variable p
data$p <- predict(logit, data, type='response')

ggplot(data = data, aes(x=age , y=p)) +
  geom_line(size=1, colour="darkgreen", linetype=2 ) +
  geom_smooth(aes(x=age , y=defaulted), method="loess", colour="orange", se=F)+
  xlab("Firm age") +
  ylab("Probability of company defaulting in 2015") +
  theme_bw()
#ggsave("logit_lpoly_age.png")
```

Please find a calibraton curve below:

```{r}
# calibration for all specific values
data[,.(mean = mean(defaulted)), by=p>0.5]

#calibration for 10 groups of p, equal # obs
data[,pcat:=cut(p,10)]
cal10 <- data[,.(mean_y = mean(defaulted), mean_p = mean(p) ), by=pcat]

#calibration for 10 groups of p, equal width
data[,pcat:=cut(p, seq(0,1,0.02), right = F,include.lowest = T )]
cal10_2 <- data[,.(mean_y = mean(defaulted), mean_p = mean(p), min = min(p) , max = max(p), n=.N ), by=pcat]

ggplot(data = cal10, aes(x=mean_p, y=mean_y)) +
  geom_line(aes(x=mean_p,y=mean_p), size=1, colour="orange",  linetype=2 )+
  geom_line(size=2, colour="darkgreen")+
  xlab("Mean predicted probability") +
  ylab("") +
  theme_bw()
#ggsave("calibration.png")
```

**Brier score**

Let's see the Brier score of the benchmark model. The Brier score is a way to measure the accuracy of the prediction. It is the mean squared difference of predicted probability and the actual outcome.

```{r}
##############################################################################
# BRIER SCORES

#logit2 <- glm(defaulted ~ 1, family = "binomial", data=data)
#data$p2 <- predict(logit2, data, type='response')

BrierScore(logit)
#BrierScore(logit2)
```

**ROC curve**

The ROC curve shows the true positive rate for a given false positive rate. There is also a line on the plot with a slope of 1, it shows the case when the are under the ROC curve is 0.5, that is the case when the prediction is not better than a random guess. The point is to cover the highest possible AUC (area under the ROC curve).

```{r}
#############################################################################
# ROC CURVE
roc(data$defaulted , data$p, plot=T)
```

In case of the benchmark model the ROC curve is not far from the 45 degree line, the prediction has to be improved.

#### 1.3 Creating training and test set

I am creating training and test data sets to train and validate the models. The training set 80% of the whole data, randomly chosen. The other 20% goes to the test set.

```{r include=FALSE}
#################################Separate a test set for final evaluation

set.seed(20180218)
# create test and train samples (80% of observations in train sample)
smp_size <- floor(0.8 * nrow(data))

# create ids: 
# 1) seq_len: generate regular sequences
# 2) sample: select random rows from a table
train_ids <- sample(seq_len(nrow(data)), size = smp_size)

# create a new variable for train/test sample
data$train <- 0
data$train[train_ids] <- 1
# Create train and test sample variables
d_train <- data[train==1,]
d_test <- data[train==0,]
```

### 2. Building models

#### 2.1 Building logit models

```{r}
#############################################################################
Y = "defaulted"
# FOUR MODELS

m1 <- get(Y) ~ age
m2 <- get(Y) ~ poly(age, 2) + region + f_ind
m3 <- get(Y) ~ poly(age, 2) + region + f_ind + size_cat + ROE_2011_cat + ROE_2012_cat + ROE_2013_cat
m4 <- get(Y) ~ age + poly(age, 2) + poly(age, 3) + region + f_ind + size_cat + sales_cat_2012 + sales_cat_2013 + income_cat_2012 + income_cat_2013 + ROE_2011_cat + ROE_2012_cat + ROE_2013_cat

#table(d_train$sales_cat_2014)
#table(d_train$income_cat_2015)
#str(d_train)
```

I am building four different logit models, first is the most simple one and the last is the most complex one. I am expecting the last to perform the best, we will see that a bit later. Logit is a regression model that has the property of yielding results between zero and one, which is its advantage over linear regressions. These are the models:

m1 ~ age

m2 ~ poly(age, 2) + region + f_ind

m3 ~ poly(age, 2) + region + f_ind + size_cat + ROE_2011_cat + ROE_2012_cat + ROE_2013_cat

m4 ~ age + poly(age, 2) + poly(age, 3) + region + f_ind + size_cat + sales_cat_2012 + sales_cat_2013 + income_cat_2012 + income_cat_2013 + ROE_2011_cat + ROE_2012_cat + ROE_2013_cat

#### 2.2 Running logit models in-sample

First I am running the models without cross-validation on the training set. Please find the results (Brier score and AUC) of the four models in the following table:

```{r}
###########################################################################
## IN-SAMPLE

in_sample_brier <- list()
in_sample_auc <- list()

for (i in 1:4){
  name <- paste("logit",2+i,sep="")
  logit <- glm(get(paste("m",i,sep='')), family = "binomial", data=d_train, na.action=na.pass)
  d_train[, (paste('p',2+i, sep=''))] <- predict(logit, d_train, type='response')
  #print(summary(logit))
  in_sample_brier[[paste0("m",i)]] <- BrierScore(logit) 
  in_sample_auc[[paste0("m",i)]] <- roc(d_train[,get(Y)] , d_train[,get(paste('p',2+i, sep=''))] , plot=F)$auc
  assign(name,logit)
}

in_sample <- cbind(in_sample_brier, in_sample_auc)
in_sample
```

The most complex model yielded the highest AUC and the best Brier score.

#### 2.3 Running logit models with 5-fold cross-validation

Now let's run the same four models on the training set but now with cross-validation.

```{r}
################################################################
## 5-FOLD CROSS-VALIDATION

set.seed(34167)

n_folds=5
# Create the folds
folds_i <- sample(rep(1:n_folds, length.out = nrow(d_train) ))

cv_results<-list()

for (i in 1:4){
  BRIER <- 0
  ROCAREA <- 0
  for (t in 1:n_folds){
    test_i <- which(folds_i == t)
    # Train sample: all except test_i
    data_train <- d_train[-test_i, ]
    # Test sample
    data_test <- d_train[test_i, ]
    logit <- glm(get(paste("m",i,sep='')), family = "binomial", data=data_train)
    p <- predict(logit, newdata=data_test , type='response')
    BRIER=BRIER + BrierScore(data_test[,get(Y)],p)
    ROCAREA=ROCAREA+ roc(data_test[,get(Y)] , p, plot=F)$auc
  }
  cv_results[[paste("model ",i,sep='')]] <- c("BRIER_SCORE"= BRIER/5 , "ROC_AREA"=ROCAREA/5)
}

cv_results
```

The results are similar in a sense that more complex models provided better results.

### 3 Using machine learning - Random Forest

Random forest is a machine learning method, growing many regression trees, 200 in our case. It uses the training sample to create many similar but slightly different samples with the use of bootstrapping. After creating 200 trees it averages out the results and yield one final tree. This procedure turned out to reduce overfitting successfully. In addition to that I am using another setting, three is set as the number of variables tried at each split.

I used the following variables for the prediction:

age + agesq + agecu + f_region + agegroup + f_ind + size_cat + sales_cat_2012 + sales_cat_2013 + income_cat_2012 + income_cat_2013 + ROE_2011_cat + ROE_2012_cat + ROE_2013_cat

The model yielded the following confusion martix:

```{r}
####################################RANDOM FOREST
mrf <- f_defaulted ~ age + agesq + agecu + f_region + agegroup + f_ind + size_cat + sales_cat_2012 + sales_cat_2013 + income_cat_2012 + income_cat_2013 + ROE_2011_cat + ROE_2012_cat + ROE_2013_cat
#?randomForest
md <- randomForest(mrf, data = d_train, ntree = 200, mtry=3)
print(md)
```

Please find a variable importance plot below. It captures the contribution of different variables to predicting power, the variable importance is plotted in a descending order. `agegroup` seems to contribute the less.

```{r}
varImpPlot(md, type = 2)
```

After training the random forest, let's try it on test set with different threshold, please find two confusion matrices below. I was trying out different thresholds, 0.05 and 0.10 respectively. The second is better in a sense that the number of false negative are much less.

```{r}
# look at two conf matrix
phat <- predict(md, data_test, type = "prob")[,"1"]
table(ifelse(phat>0.05,1,0), data_test$f_defaulted)
table(ifelse(phat>0.10,1,0), data_test$f_defaulted)
```

#### 4. Comparing results

Now I am going to compare the performance of the best logit model and the random forest forest method on the test set. The Brier score of the logit model is the following. It is lower 

```{r}
################################################

# for comparison, train-test logits

logit_best <- glm(m4, family = "binomial", data=d_train)
p <- predict(logit_best, newdata=d_test , type='response')
BrierScore(d_test[,get(Y)],p)
```

The logit model has the provided the following ROC curve. It seems to provide a high AUC value.

```{r}
plot.roc(d_test[,get(Y)] , p, print.auc = TRUE)
```

The random forest gave the following ROC curve. Compared to logit model it provided a lower AUC value.

```{r}
model.test.prob <- predict(md, d_test, type = "prob",norm.votes = TRUE)
plot.roc(predictor = model.test.prob[,2],  d_test$f_defaulted, print.auc = TRUE)
```

